import feedparser
import json
import os
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta
import glob

# ä¿åº•é“¾æ¥
MANUAL_FEEDS = [
    "https://feed.xyzfm.space/dk4yh3pkpjp3"
]

# å…¨å±€è¯Šæ–­æ—¥å¿—
debug_log = []

def log(msg):
    print(msg)
    debug_log.append(msg)

def find_opml_file(root_dir):
    # 1. å°è¯•æ ‡å‡†è·¯å¾„
    standard_path = os.path.join(root_dir, 'subscriptions.opml')
    if os.path.exists(standard_path):
        return standard_path
    
    # 2. å°è¯•ä¸åŒºåˆ†å¤§å°å†™æœç´¢
    files = os.listdir(root_dir)
    for f in files:
        if f.lower().endswith('.opml'):
            return os.path.join(root_dir, f)
            
    return None

def parse_opml(opml_path):
    urls = []
    try:
        tree = ET.parse(opml_path)
        root = tree.getroot()
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰ outline æ ‡ç­¾
        for outline in root.findall('.//outline'):
            # å°è¯•å¤šç§å±æ€§å (æœ‰çš„ OPML ç”¨ xmlUrlï¼Œæœ‰çš„ç”¨ url)
            url = outline.get('xmlUrl') or outline.get('url')
            if url:
                urls.append(url)
        log(f"ğŸ“‚ è§£æ OPML æˆåŠŸ: æ‰¾åˆ° {len(urls)} ä¸ªè®¢é˜…æº")
    except Exception as e:
        log(f"âš ï¸ è§£æ OPML å‡ºé”™: {str(e)}")
    return urls

def get_best_link(entry):
    if entry.get('link'): return entry.get('link')
    if entry.get('links'):
        for l in entry.get('links', []):
            if l.get('type') == 'text/html' or l.get('rel') == 'alternate':
                if l.get('href'): return l.get('href')
    id_val = entry.get('id', '')
    if id_val.startswith('http'): return id_val
    if entry.get('enclosures'): return entry.get('enclosures')[0].get('href')
    return ''

def fetch_rss():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    root_dir = os.path.dirname(current_dir)
    data_dir = os.path.join(root_dir, 'data')
    
    log(f"ğŸ“ è„šæœ¬è¿è¡Œç›®å½•: {current_dir}")
    log(f"ğŸ  é¡¹ç›®æ ¹ç›®å½•: {root_dir}")
    
    # æŸ¥æ‰¾ OPML
    opml_path = find_opml_file(root_dir)
    
    rss_feeds = []
    if opml_path:
        log(f"âœ… æ‰¾åˆ° OPML æ–‡ä»¶: {opml_path}")
        rss_feeds = parse_opml(opml_path)
        if not rss_feeds:
            log("âš ï¸ OPML æ–‡ä»¶æ˜¯ç©ºçš„æˆ–æ ¼å¼ä¸å¯¹ï¼Œä½¿ç”¨ä¿åº•åˆ—è¡¨")
            rss_feeds = MANUAL_FEEDS
    else:
        log("âŒ æœªæ‰¾åˆ°ä»»ä½• .opml æ–‡ä»¶ï¼å°†åœ¨æ ¹ç›®å½•ä¸‹åˆ—å‡ºæ‰€æœ‰æ–‡ä»¶ä»¥ä¾›æ’æŸ¥:")
        try:
            files = os.listdir(root_dir)
            log(f"ğŸ“„ æ ¹ç›®å½•æ–‡ä»¶åˆ—è¡¨: {', '.join(files)}")
        except:
            pass
        log("ğŸ‘‰ ä½¿ç”¨ä¿åº•åˆ—è¡¨")
        rss_feeds = MANUAL_FEEDS

    if not os.path.exists(data_dir):
        os.makedirs(data_dir)

    all_episodes = []
    # æŠ“å–è¿‡å» 7 å¤©
    time_threshold = datetime.now() - timedelta(days=7)

    log(f"ğŸš€ å¼€å§‹æŠ“å– {len(rss_feeds)} ä¸ªæº...")

    for feed_url in rss_feeds:
        try:
            feed = feedparser.parse(feed_url)
            podcast_title = feed.feed.get('title', 'æœªçŸ¥æ’­å®¢')
            
            if not feed.entries:
                continue

            for entry in feed.entries:
                try:
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date_parsed = entry.published_parsed
                        pub_date = datetime(*pub_date_parsed[:6])
                    else:
                        continue
                except:
                    continue

                if pub_date > time_threshold:
                    final_link = get_best_link(entry)
                    all_episodes.append({
                        'title': entry.title,
                        'podcast': {'title': podcast_title},
                        'link': final_link,
                        'pubDate': str(pub_date)
                    })
        except Exception as e:
            # å•ä¸ªæºå¤±è´¥ä¸è®°å½•åˆ°å…¨å±€æ—¥å¿—ï¼Œå…å¾—å¤ªé•¿
            print(f"âŒ é”™è¯¯ {feed_url}: {e}")

    all_episodes.sort(key=lambda x: x['pubDate'], reverse=True)

    # ä¿å­˜æ•°æ®
    output_file = os.path.join(data_dir, 'hot_episodes.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_episodes, f, ensure_ascii=False, indent=2)

    # ==========================================
    # é‡ç‚¹ï¼šæŠŠè¯Šæ–­æ—¥å¿—ä¹Ÿä¿å­˜ä¸‹æ¥ï¼Œç¨åå‘é‚®ä»¶ç”¨
    # ==========================================
    debug_file = os.path.join(root_dir, 'debug_log.txt')
    with open(debug_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(debug_log))
    
    print(f"\nğŸ‰ å®Œæˆï¼æŠ“å–åˆ° {len(all_episodes)} é›†ã€‚")

if __name__ == "__main__":
    fetch_rss()

