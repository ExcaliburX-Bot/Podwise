import feedparser
import json
import os
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta

# ==========================================
# å¤‡ç”¨æ‰‹åŠ¨åˆ—è¡¨ (å¦‚æœæ²¡æœ‰ä¸Šä¼  OPML æ–‡ä»¶ï¼Œä¼šç”¨è¿™ä¸ª)
MANUAL_FEEDS = [
    "https://feeds.xyz/example1",
    "https://feeds.xyz/example2",
]
# ==========================================

def parse_opml(opml_path):
    """è§£æ OPML æ–‡ä»¶æå– RSS é“¾æ¥"""
    urls = []
    try:
        tree = ET.parse(opml_path)
        root = tree.getroot()
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰ outline æ ‡ç­¾
        for outline in root.findall('.//outline'):
            # æ ‡å‡† OPML é€šå¸¸æŠŠé“¾æ¥æ”¾åœ¨ xmlUrl å±æ€§é‡Œ
            url = outline.get('xmlUrl')
            if url:
                urls.append(url)
        print(f"ğŸ“‚ æˆåŠŸä» OPML åŠ è½½äº† {len(urls)} ä¸ªè®¢é˜…æº")
    except Exception as e:
        print(f"âš ï¸ è¯»å– OPML å¤±è´¥: {e}")
    return urls

def get_best_link(entry):
    """å¼ºåŠ›æå–é“¾æ¥é€»è¾‘"""
    if entry.get('link'): return entry.get('link')
    if entry.get('links'):
        for l in entry.get('links', []):
            if l.get('type') == 'text/html' or l.get('rel') == 'alternate':
                if l.get('href'): return l.get('href')
    id_val = entry.get('id', '')
    if id_val.startswith('http'): return id_val
    if entry.get('enclosures'): return entry.get('enclosures')[0].get('href')
    return ''

def fetch_rss():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    root_dir = os.path.dirname(current_dir)
    data_dir = os.path.join(root_dir, 'data')
    
    # 1. å°è¯•å¯»æ‰¾æ ¹ç›®å½•ä¸‹çš„ subscriptions.opml
    opml_path = os.path.join(root_dir, 'subscriptions.opml')
    
    rss_feeds = []
    if os.path.exists(opml_path):
        rss_feeds = parse_opml(opml_path)
    else:
        print("â„¹ï¸ æœªæ‰¾åˆ° subscriptions.opmlï¼Œä½¿ç”¨æ‰‹åŠ¨åˆ—è¡¨")
        rss_feeds = MANUAL_FEEDS

    if not os.path.exists(data_dir):
        os.makedirs(data_dir)

    all_episodes = []
    yesterday = datetime.now() - timedelta(days=1)

    print(f"ğŸš€ å¼€å§‹å¤„ç† {len(rss_feeds)} ä¸ªè®¢é˜…æº...")

    for feed_url in rss_feeds:
        try:
            # è®¾ç½®è¶…æ—¶ï¼Œé˜²æ­¢å¡æ­»
            feed = feedparser.parse(feed_url)
            podcast_title = feed.feed.get('title', 'æœªçŸ¥æ’­å®¢')
            
            # ç®€å•çš„æ—¥å¿—è¾“å‡ºï¼Œé¿å…åˆ·å±
            # print(f"Checking: {podcast_title}") 

            for entry in feed.entries:
                try:
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date_parsed = entry.published_parsed
                        pub_date = datetime(*pub_date_parsed[:6])
                    else:
                        continue
                except:
                    continue

                if pub_date > yesterday:
                    final_link = get_best_link(entry)
                    
                    print(f"   âœ… æ–°æ›´æ–°: {podcast_title} - {entry.title[:20]}...")

                    all_episodes.append({
                        'title': entry.title,
                        'podcast': {'title': podcast_title},
                        'link': final_link,
                        'pubDate': str(pub_date)
                    })
        except Exception as e:
            print(f"âŒ é”™è¯¯ {feed_url}: {e}")

    output_file = os.path.join(data_dir, 'hot_episodes.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_episodes, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼å…±æŠ“å–åˆ° {len(all_episodes)} ä¸ªæ–°å•é›†ã€‚")

if __name__ == "__main__":
    fetch_rss()
