import feedparser
import json
import os
import xml.etree.ElementTree as ET
from datetime import datetime, timedelta

# ==========================================
# 1. è¿™é‡ŒåŠ äº†ä¸€ä¸ªçœŸå®çš„ RSS (æœºæ ¸ç½‘)ï¼Œä¿è¯æµ‹è¯•æ—¶ä¸€å®šæœ‰æ•°æ®ï¼
MANUAL_FEEDS = [
    "https://feed.xyz/example1", 
    "https://www.gcores.com/rss",  # <--- è¿™æ˜¯ä¸€ä¸ªçœŸå®çš„æµ‹è¯•æº
]
# ==========================================

def parse_opml(opml_path):
    urls = []
    try:
        tree = ET.parse(opml_path)
        root = tree.getroot()
        for outline in root.findall('.//outline'):
            url = outline.get('xmlUrl')
            if url:
                urls.append(url)
        print(f"ğŸ“‚ æˆåŠŸä» OPML åŠ è½½äº† {len(urls)} ä¸ªè®¢é˜…æº")
    except Exception as e:
        print(f"âš ï¸ è¯»å– OPML å¤±è´¥: {e}")
    return urls

def get_best_link(entry):
    if entry.get('link'): return entry.get('link')
    if entry.get('links'):
        for l in entry.get('links', []):
            if l.get('type') == 'text/html' or l.get('rel') == 'alternate':
                if l.get('href'): return l.get('href')
    id_val = entry.get('id', '')
    if id_val.startswith('http'): return id_val
    if entry.get('enclosures'): return entry.get('enclosures')[0].get('href')
    return ''

def fetch_rss():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    root_dir = os.path.dirname(current_dir)
    data_dir = os.path.join(root_dir, 'data')
    
    opml_path = os.path.join(root_dir, 'subscriptions.opml')
    
    rss_feeds = []
    if os.path.exists(opml_path):
        rss_feeds = parse_opml(opml_path)
    else:
        print("â„¹ï¸ æœªæ‰¾åˆ° subscriptions.opmlï¼Œä½¿ç”¨æ‰‹åŠ¨åˆ—è¡¨ (å«æµ‹è¯•æº)")
        rss_feeds = MANUAL_FEEDS

    if not os.path.exists(data_dir):
        os.makedirs(data_dir)

    all_episodes = []
    
    # ==========================================
    # 2. ä¿®æ”¹è¿™é‡Œï¼šæŠŠ days=1 æ”¹æˆ days=7 (æŠ“å–è¿‡å»ä¸€å‘¨çš„)
    time_threshold = datetime.now() - timedelta(days=7)
    # ==========================================

    print(f"ğŸš€ å¼€å§‹å¤„ç† {len(rss_feeds)} ä¸ªè®¢é˜…æº (æŸ¥æ‰¾ {time_threshold.strftime('%Y-%m-%d')} ä¹‹åçš„æ›´æ–°)...")

    for feed_url in rss_feeds:
        try:
            feed = feedparser.parse(feed_url)
            podcast_title = feed.feed.get('title', 'æœªçŸ¥æ’­å®¢')
            
            for entry in feed.entries:
                try:
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date_parsed = entry.published_parsed
                        pub_date = datetime(*pub_date_parsed[:6])
                    else:
                        continue
                except:
                    continue

                # ä½¿ç”¨æ–°çš„ 7 å¤©æ—¶é—´é˜ˆå€¼
                if pub_date > time_threshold:
                    final_link = get_best_link(entry)
                    print(f"   âœ… æŠ“å–åˆ°: {podcast_title} - {entry.title[:15]}...")

                    all_episodes.append({
                        'title': entry.title,
                        'podcast': {'title': podcast_title},
                        'link': final_link,
                        'pubDate': str(pub_date)
                    })
        except Exception as e:
            print(f"âŒ é”™è¯¯ {feed_url}: {e}")

    output_file = os.path.join(data_dir, 'hot_episodes.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_episodes, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ‰ å…¨éƒ¨å®Œæˆï¼å…±æŠ“å–åˆ° {len(all_episodes)} ä¸ªæ–°å•é›†ã€‚")

if __name__ == "__main__":
    fetch_rss()
