import feedparser
import json
import os
import re  # ğŸ‘ˆ å¼•å…¥æ­£åˆ™åº“
from datetime import datetime, timedelta

# ä¿åº•é“¾æ¥
MANUAL_FEEDS = [
    "https://feed.xyzfm.space/dk4yh3pkpjp3"
]

# å…¨å±€è¯Šæ–­æ—¥å¿—
debug_log = []

def log(msg):
    print(msg)
    debug_log.append(msg)

def find_opml_file(root_dir):
    standard_path = os.path.join(root_dir, 'subscriptions.opml')
    if os.path.exists(standard_path):
        return standard_path
    files = os.listdir(root_dir)
    for f in files:
        if f.lower().endswith('.opml'):
            return os.path.join(root_dir, f)
    return None

def parse_opml(opml_path):
    urls = []
    try:
        # ğŸ“– ä½¿ç”¨â€œæš´åŠ›æ¨¡å¼â€è¯»å–ï¼Œå¿½ç•¥ XML è¯­æ³•é”™è¯¯
        with open(opml_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç›´æ¥æå– http/https é“¾æ¥
            # ä¸“é—¨åŒ¹é… xmlUrl="..." æˆ– url="..." é‡Œçš„å†…å®¹
            matches = re.findall(r'(?:xmlUrl|url)=["\'](http[^"\']+)["\']', content)
            
            # å»é‡
            urls = list(set(matches))
            
        log(f"ğŸ“‚ è§£æ OPML æˆåŠŸ (æ­£åˆ™æš´åŠ›æ¨¡å¼): æ‰¾åˆ° {len(urls)} ä¸ªè®¢é˜…æº")
    except Exception as e:
        log(f"âš ï¸ è§£æ OPML å¤±è´¥: {str(e)}")
    return urls

def get_best_link(entry):
    if entry.get('link'): return entry.get('link')
    if entry.get('links'):
        for l in entry.get('links', []):
            if l.get('type') == 'text/html' or l.get('rel') == 'alternate':
                if l.get('href'): return l.get('href')
    id_val = entry.get('id', '')
    if id_val.startswith('http'): return id_val
    if entry.get('enclosures'): return entry.get('enclosures')[0].get('href')
    return ''

def fetch_rss():
    current_dir = os.path.dirname(os.path.abspath(__file__))
    root_dir = os.path.dirname(current_dir)
    data_dir = os.path.join(root_dir, 'data')
    
    log(f"ğŸ“ è„šæœ¬è¿è¡Œç›®å½•: {current_dir}")
    
    opml_path = find_opml_file(root_dir)
    
    rss_feeds = []
    if opml_path:
        log(f"âœ… æ‰¾åˆ° OPML æ–‡ä»¶: {opml_path}")
        rss_feeds = parse_opml(opml_path)
        if not rss_feeds:
            log("âš ï¸ OPML æå–ç»“æœä¸ºç©ºï¼Œä½¿ç”¨ä¿åº•åˆ—è¡¨")
            rss_feeds = MANUAL_FEEDS
    else:
        log("âŒ æœªæ‰¾åˆ°ä»»ä½• .opml æ–‡ä»¶ï¼Œä½¿ç”¨ä¿åº•åˆ—è¡¨")
        rss_feeds = MANUAL_FEEDS

    if not os.path.exists(data_dir):
        os.makedirs(data_dir)

    all_episodes = []
    # æŠ“å–è¿‡å» 7 å¤©
    time_threshold = datetime.now() - timedelta(days=7)

    log(f"ğŸš€ å¼€å§‹æŠ“å– {len(rss_feeds)} ä¸ªæº (è¿‡å» 7 å¤©)...")

    # é™åˆ¶æœ€å¤§æŠ“å–æ•°é‡ï¼Œé˜²æ­¢ GitHub Action è¶…æ—¶ï¼ˆå¦‚æœæºå¤ªå¤šï¼‰
    # å¦‚æœä½ çš„æºè¶…è¿‡ 100 ä¸ªï¼Œå¯ä»¥é€‚å½“è°ƒå¤§è¿™ä¸ªæ•°å­—ï¼Œæˆ–è€…åˆ†æ‰¹å¤„ç†
    max_feeds = 200 
    if len(rss_feeds) > max_feeds:
        log(f"âš ï¸ æºå¤ªå¤š ({len(rss_feeds)} ä¸ª)ï¼Œä»…å¤„ç†å‰ {max_feeds} ä¸ªä»¥é˜²è¶…æ—¶")
        rss_feeds = rss_feeds[:max_feeds]

    success_count = 0
    
    for feed_url in rss_feeds:
        try:
            # è®¾ç½®è¶…æ—¶æ—¶é—´ 10ç§’ï¼Œé˜²æ­¢å¡æ­»
            # æ³¨æ„ï¼šfeedparser æœ¬èº«ä¸æ”¯æŒ timeout å‚æ•°ï¼Œè¿™é‡Œä¾èµ– socket é»˜è®¤è¶…æ—¶æˆ–å¿«é€Ÿå¤±è´¥
            feed = feedparser.parse(feed_url)
            
            if not feed.entries:
                continue
                
            podcast_title = feed.feed.get('title', 'æœªçŸ¥æ’­å®¢')
            success_count += 1

            for entry in feed.entries:
                try:
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date_parsed = entry.published_parsed
                        pub_date = datetime(*pub_date_parsed[:6])
                    else:
                        continue
                except:
                    continue

                if pub_date > time_threshold:
                    final_link = get_best_link(entry)
                    all_episodes.append({
                        'title': entry.title,
                        'podcast': {'title': podcast_title},
                        'link': final_link,
                        'pubDate': str(pub_date)
                    })
        except Exception as e:
            # è¿™é‡Œçš„é”™è¯¯å°±ä¸æ‰“å°äº†ï¼Œä¿æŒæ—¥å¿—æ¸…çˆ½
            pass

    log(f"âœ… æˆåŠŸè¿æ¥å¹¶è§£æäº† {success_count} ä¸ªæ’­å®¢æº")

    all_episodes.sort(key=lambda x: x['pubDate'], reverse=True)

    output_file = os.path.join(data_dir, 'hot_episodes.json')
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_episodes, f, ensure_ascii=False, indent=2)

    debug_file = os.path.join(root_dir, 'debug_log.txt')
    with open(debug_file, 'w', encoding='utf-8') as f:
        f.write('\n'.join(debug_log))
    
    print(f"\nğŸ‰ å®Œæˆï¼æŠ“å–åˆ° {len(all_episodes)} é›†ã€‚")

if __name__ == "__main__":
    fetch_rss()
